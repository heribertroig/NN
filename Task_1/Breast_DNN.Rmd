---
title: "Breast"
author: "Oriol Planesas, Heribert Roig"
date: "`r format(Sys.Date(),'%e de %B, %Y')`"
output:
  pdf_document: default
  html_document: default
params:
  file1: "protein_abundance.csv"
  file2: "gene_expression.csv"
  file3: "clinical.csv"
  file4: "copy_number.csv"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results='hide'}
library(keras)
library(caret)
library(pROC)
```

**1. Describe protein abundance and gene expression datasets. How many patients have data of both types available. Are there missing data from some of the datasets? Preprocess them if necessary.**

```{r data import}
protein_abundance <- read.csv(params$file1, sep = "")
gene <- read.csv(params$file2, sep = "")
clinical <- read.csv(params$file3, sep = "\t")
copy <- read.csv(params$file4, sep = "\t")
```

Haremos un pequeño resumen de los primeros genes de las diversas bases de datos para ver en que consisten estos:

```{r}
summary(gene[,1:10])
summary(clinical[,1:4])
summary(protein_abundance[,1:10])
```

Vemos como las bases de datos de protein y gene consisten en valores numericos con valores bajos alrededor del 0 tanto en positivo como negativo. También observamos como aquellos genes y proteinas con valores similares tienen también un nonmbre parecido, por consecuente, parece que aquellos genes relacionados entre si o que son parecidos tienen un efecto similar.

```{r Comparten individuos}

set1<-intersect(protein_abundance$Sample,gene$Sample)
set1 <- intersect(set1, clinical$Sample)

xgene<-gene[gene$Sample %in% set1,]
xprotein<-protein_abundance[protein_abundance$Sample %in% set1,]

xclinical <- clinical[clinical$Sample %in% set1,]
xclinical <- xclinical[,c(1,9)]

dim(xgene)
```

387 individuos están presentes en los datasets clinical, gene y protein_abundance.

```{r missing}
sum(is.na(xgene)) # 1161 missings en gene
sum(is.na(xprotein)) # 0 missings en protein
sum(is.na(xclinical)) # 0 missings en protein

```

Podemos observar como en la base de datos "gene_expression" hay 1161 valores missing, que arreglaremos mas adelante.

# With gene expression data:

**2. Select the 25% of genes with the most variability**

```{r}
gene_var <- diag(var(gene[,2:ncol(gene)], na.rm = T)) # Calculamos la variabilidad de los genes

gene_topvar <- sort(gene_var, decreasing = T)[1:(length(gene_var)/20)]
# Cogemos solo el 5% con la mayor variabilidad para no tener problemas con los modelos

# Separamos los 5% de genes con mas variabilidad

xgene <- xgene[,c("Sample", names(gene_topvar))]

siNAcol <- apply(is.na(xgene), 2, sum) >= 1

xgene <- xgene[,!siNAcol]

sum(is.na(xgene)) # No hi ha missings

dim(xgene)
```
En lugar de utilizar el 25% de genes con más variabilidad, utilizaremos sólo el 5% debido a que el número de variables es relativamente grande para ejecutar el programa en nuestros ordenadores personales.Asimismo, obtenemos un conjunto de datos sin missings.

Los datos de gene expression que utilizaremos tienen una dimension final de 387 observaciones y 890 variables.

```{r}
# Eliminamos las filas donde la respuesta no es ni negativa ni positiva:
sel1<-which(xclinical$breast_carcinoma_estrogen_receptor_status != "Positive")
sel2<-which(xclinical$breast_carcinoma_estrogen_receptor_status != "Negative")
sel<-intersect(sel1,sel2)
xclinical<-xclinical[-sel,]
data1 <- merge(xclinical, xgene, by.x = "Sample", by.y = "Sample")
```

Generamos los datos de training y test para los modelos que vengan a continuacion de la base de datos gene:

```{r}
set.seed(123)
training<-sample(1:nrow(data1),2*nrow(data1)/3)

escalat1 <- scale(data1[,-c(1,2)])

xtrain<-escalat1[training,]
xtest<-escalat1[-training,]
ytrain<-data1[training,2]
ytest<-data1[-training,2]
ylabels<-vector()
ylabels[ytrain=="Positive"]<-1
ylabels[ytrain=="Negative"]<-0
ylabelstest<-vector()
ylabelstest[ytest=="Positive"]<-1
ylabelstest[ytest=="Negative"]<-0

```

**3. Implement an stacked autoencoder (SAE) with three stacked layers of 1000, 100, 50 nodes. Provide in each case evidence of the quality of the coding obtained.**

Empezaremos generando el primer autoencoder con un shape de el numero de columnas nunmericas que hay en la base de datos xgene, que es el mismo numero de columnas que tiene el train, en el primer layer_input con los datos de "gene_expression". El primer decode tendra 1000 nodos y asi succesivamente con los valores dados en el enunciado.

```{r}
# Autoencoder 1

# Encoder
input_enc1 <- layer_input(shape = (ncol(xgene) - 1))
output_enc1 <- input_enc1 %>%
  layer_dense(units = 1000, activation = "relu", name='G_Enc1')
encoder1 = keras_model(input_enc1, output_enc1)
summary(encoder1)

# Decoder
input_dec1 = layer_input(shape = 1000)
output_dec1 <- input_dec1 %>%
  layer_dense(units = (ncol(xgene)-1), activation="linear", name='G_Dec1')
decoder1 = keras_model(input_dec1, output_dec1)
summary(decoder1)

# Juntar el encoder y el decoder
aen_input1 = layer_input(shape = (ncol(xgene)-1))
aen_output1 = aen_input1 %>%
  encoder1() %>%
  decoder1()
sae1 = keras_model(aen_input1, aen_output1)
summary(sae1)

sae1 %>% compile(
optimizer = "rmsprop",
loss = "mse")

sae1 %>% fit(
x=as.matrix(xtrain),
y=as.matrix(xtrain),
epochs = 25,
batch_size=64,
validation_split = 0.2)

#Generador con en encoder
encoded_expression1 <- encoder1 %>% predict(as.matrix(xtrain))

```
El primer autoencoder tiene 1779889 parámetros.

```{r}
# Autoencoder 2
input_enc2 <- layer_input(shape = 1000)
output_enc2 <- input_enc2 %>%
  layer_dense(units = 100, activation = "relu", name='Enc_AE2')
encoder2 = keras_model(input_enc2, output_enc2)
summary(encoder2)

input_dec2 = layer_input(shape = 100)
output_dec2 <- input_dec2 %>%
  layer_dense(units = 1000, activation="linear", name='Dec_AE1')
decoder2 = keras_model(input_dec2, output_dec2)
summary(decoder2)

aen_input2 = input_enc2
aen_output2 = aen_input2 %>%
  encoder2() %>%
  decoder2()
sae2 = keras_model(aen_input2, aen_output2)
summary(sae2)


sae2 %>% compile(
optimizer = "rmsprop",
loss = "mse")

sae2 %>% fit(
x=as.matrix(encoded_expression1),
y=as.matrix(encoded_expression1),
epochs = 25,
batch_size=64,
validation_split = 0.2)

encoded_expression2 <- encoder2 %>% predict(as.matrix(encoded_expression1))

```
Este autoencoder tiene 201100 parámetros

```{r}
# Autoencoder 3

# Encoder
input_enc3 <- layer_input(shape = 100)
output_enc3 <- input_enc3 %>%
  layer_dense(units = 50, activation = "relu", name='Enc_AE3')
encoder3 = keras_model(input_enc3, output_enc3)
summary(encoder3)

# Decoder
input_dec3 = layer_input(shape = 50)
output_dec3 <- input_dec3 %>%
  layer_dense(units = 100, activation="linear", name='Dec_AE1')
decoder3 = keras_model(input_dec3, output_dec3)
summary(decoder3)

aen_input3 = input_enc3
aen_output3 = aen_input3 %>%
  encoder3() %>%
  decoder3()
sae3 = keras_model(aen_input3, aen_output3)
summary(sae3)

sae3 %>% compile(
optimizer = "rmsprop",
loss = "mse")

sae3 %>% fit(
x=as.matrix(encoded_expression2),
y=as.matrix(encoded_expression2),
epochs = 40,
batch_size=64,
validation_split = 0.2)

encoded_expression3 <- encoder3 %>% predict(as.matrix(encoded_expression2))


```

El tercer autoencoder tiene 10150 parámetros.

**4.Using the SAE as pre-training model, couple it with a two-layer DNN to predict the state of the estrogen receptor. The DNN must have 10 nodes in the first layer followed by the output layer.**

Generamos el modelo juntando los 3 encoders anteriores

```{r}
sae_input = layer_input(shape = (ncol(xgene)-1), name = "input_gene")
sae_output = sae_input %>%
  encoder1() %>%
  encoder2() %>%
  encoder3() %>%
  layer_dense(10,activation = "relu", name='L1_SAE1')%>%
  layer_dense(1,activation = "sigmoid", name='L2_SAE1')
sae = keras_model(sae_input, sae_output)
summary(sae)

freeze_weights(sae,from=1,to=3)

```

Al juntar todos los encoder en un mismo modelo tenemos que este acaba teniendo 995671 parametros.

```{r}
sae %>% compile(
optimizer = "rmsprop",
loss = 'binary_crossentropy',
metric = "acc"
)
```


```{r}
sae %>% fit(
x=xtrain,
y=ylabels,
epochs = 15,
batch_size = 64,
validation_split = 0.2
)

```
El valor de accuracy que obtenemos es cercano a 0.85 y la pérdida es aproximadamente 0.4

```{r}
sae %>% evaluate(as.matrix(xtest), ylabelstest)
```

Cuando evaluamos el modelo con los datos de test, conseguimos unos valores de las métricas de loss y accuracy muy parecidos a los de entrenamiento, por lo que podemos decir que tenemos un buen rendimiento en el modelo.

```{r}
yhat <- predict(sae,as.matrix(xtest))
```


```{r}
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
table(yhatclass, ylabelstest)
```

```{r}
confusionMatrix(yhatclass,as.factor(ylabelstest))
```

Vemos que al predecir valores con la prediccion observamos como el modelo tiene mayor error a la hora de predecir los casos negativos (0). Puede ser debido a que hay un número mayor de muestras con respuesta positiva, por lo que el modelo está más entrenado para este caso.

**5.On the test set, provide the ROC curve and AUC and other performance metrics.**

```{r}
roc_sae_test <- roc(response = ylabelstest, predictor =yhat)
```


```{r}
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

En este grafico para ver el valor de auc obtenemos que este valor es de 0.915, teniendo asi que este modelo tiene un buen valor de diagnostico.

**6. With tfruns() repeat points 4 and 5, exploring the configurations of the first layer of the DNN based on 5, 10 and 20 nodes. Determine which configuration is the best.**

Para realizar el tfruns, generaremos el código en otro archivo .R y entonces cargaremos aqui los diferentes modelos con el codigo que hay a continuacion.

```{r}
library(tfruns)
nodes <- c(5, 10, 20)

for (i in 1:length(nodes)){
  print(i)
  training_run("Breast_tfruns.R",
               flags = c(units = nodes[i]))
}

runs <- ls_runs(latest_n = 3)
runs <- runs[, c("flag_units", "metric_val_acc", "metric_val_loss")]
(runs <- runs[order(runs$metric_val_acc, decreasing = T),])

```

Vemos que los tres modelos ejecutados tienen un accuracy parecido, por lo que en los siguientes ejercicios utilizaremos la configuración inicial.

**So far, we have two SAEs. One for the abundance of proteins (see class examples) and the other for gene expression we just built.**

**7. Split the set of patients with complete data (gene expression and protein abundance) in train and test sets.**

```{r}
xprotein<-protein_abundance[protein_abundance$Sample %in% set1,]
```


```{r}
data2 <- merge(xclinical, xprotein, by.x = "Sample", by.y = "Sample")
data2 <- merge(data2, xgene, by.x = "Sample", by.y = "Sample")
```


```{r}
escalat2 <- scale(data2[,-c(1,2)])


xtrain2<-escalat2[training,-c(1,2)]
xtest2<-escalat2[-training,-c(1,2)]
xtrain2<-scale(xtrain2)
xtest2<-scale(xtest2)
ytrain2<-escalat2[training,2]
ytest2<-escalat2[-training,2]
ylabels2<-vector()
ylabels2[ytrain2=="Positive"]<-1
ylabels2[ytrain2=="Negative"]<-0
ylabelstest2<-vector()
ylabelstest2[ytest2=="Positive"]<-1
ylabelstest2[ytest2=="Negative"]<-0

```

**8. Concatenate the two SAEs to fit, on the trainset, a DNN that integrates both data sources to predict estrogen receptor status. The DNN must have a dense layer (with the better number of nodes according with point 6) and the output layer.**

Modelo de la proteina


```{r}
data3<-merge(xclinical,xprotein,by.x="Sample",by.y="Sample")
```


```{r}
escalat3 <- scale(data3[,-c(1,2)])

xtrain3<-escalat3[training,]
xtest3<-escalat3[-training,]

ytrain3<-data3[training,2]
ytest3<-data3[-training,2]

ylabels3<-vector()
ylabels3[ytrain3=="Positive"]<-1
ylabels3[ytrain3=="Negative"]<-0


ytestlabels3<-vector()
ytestlabels3[ytest3=="Positive"]<-1
ytestlabels3[ytest3=="Negative"]<-0
```


```{r}
# AE1
input_enc1_prot<-layer_input(shape = 142)
output_enc1_prot<-input_enc1_prot %>% 
  layer_dense(units=50,activation="relu") 
encoder1_prot = keras_model(input_enc1_prot, output_enc1_prot, name = "AE1")
summary(encoder1_prot)

input_dec1_prot = layer_input(shape = 50)
output_dec1_prot<-input_dec1_prot %>% 
  layer_dense(units=142,activation="linear")

decoder1_prot = keras_model(input_dec1_prot, output_dec1_prot)
 
summary(decoder1_prot)

aen_input1_prot = layer_input(shape = 142)
aen_output1_prot = aen_input1_prot %>% 
  encoder1_prot() %>% 
  decoder1_prot()
   
sae1_prot = keras_model(aen_input1_prot, aen_output1_prot)
summary(sae1_prot)

sae1_prot %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)

sae1_prot %>% fit(
  x=as.matrix(xtrain3),
  y=as.matrix(xtrain3),
  epochs = 50,
  batch_size=64,
  validation_split = 0.2
  )

#Generating with Autoencoder
encoded_expression1_prot <- encoder1_prot %>% predict(as.matrix(xtrain3))
```

El primer autoencoder tiene 14392 parámetros

```{r}
# AE2
input_enc2_prot<-layer_input(shape = 50)
output_enc2_prot<-input_enc2_prot %>% 
  layer_dense(units=20,activation="relu") 
encoder2_prot = keras_model(input_enc2_prot, output_enc2_prot)
summary(encoder2_prot)

input_dec2_prot = layer_input(shape = 20)
output_dec2_prot<-input_dec2_prot %>% 
  layer_dense(units=50,activation="linear")

decoder2_prot = keras_model(input_dec2_prot, output_dec2_prot)
 
summary(decoder2_prot)

aen_input2_prot = layer_input(shape = 50)
aen_output2_prot = aen_input2_prot %>% 
  encoder2_prot() %>% 
  decoder2_prot()
   
sae2_prot = keras_model(aen_input2_prot, aen_output2_prot)
summary(sae2_prot)


sae2_prot %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)

sae2_prot %>% fit(
  x=as.matrix(encoded_expression1_prot),
  y=as.matrix(encoded_expression1_prot),
  epochs = 50,
  batch_size=64,
  validation_split = 0.2
  )

#Generating with Autoencoder
encoded_expression2_prot <- encoder2_prot %>% predict(as.matrix(encoded_expression1_prot))
```

El segundo autoencoder tiene 2070 parámetros.

```{r}
# AE3
input_enc3_prot<-layer_input(shape = 20)
output_enc3_prot<-input_enc3_prot %>% 
  layer_dense(units=10,activation="relu") 
encoder3_prot = keras_model(input_enc3_prot, output_enc3_prot)
summary(encoder3_prot)

input_dec3_prot = layer_input(shape = 10)
output_dec3_prot<-input_dec3_prot %>% 
  layer_dense(units=20,activation="linear")

decoder3_prot = keras_model(input_dec3_prot, output_dec3_prot)
 
summary(decoder3_prot)

aen_input3_prot = layer_input(shape = 20)
aen_output3_prot = aen_input3_prot %>% 
  encoder3_prot() %>% 
  decoder3_prot()
   
sae3_prot = keras_model(aen_input3_prot, aen_output3_prot)
summary(sae3_prot)

sae3_prot %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)

sae3_prot %>% fit(
  x=as.matrix(encoded_expression2_prot),
  y=as.matrix(encoded_expression2_prot),
  epochs = 50,
  batch_size=64,
  validation_split = 0.2
  )


#Generating with Autoencoder
encoded_expression3_prot <- encoder3_prot %>% predict(as.matrix(encoded_expression2_prot))
```

El tercer autoencoder para el conjunto de datos proteicos tiene 430 parámetros.



```{r}
### Final model

sae_input_prot = layer_input(shape = 142, name = "input_prot")
sae_output_prot = sae_input_prot %>% 
  encoder1_prot() %>% 
  encoder2_prot()  %>%
  encoder3_prot() %>%
  layer_dense(5,activation = "relu")%>%
  layer_dense(1,activation = "sigmoid")
   
sae_prot = keras_model(sae_input_prot, sae_output_prot)
summary(sae_prot)
```
El total de parámetros para el modelo Stacked autoencoder para el conjunto de datos de protein_abundance es de 8170.

```{r}
freeze_weights(sae_prot,from=1,to=3)
```

```{r}
sae_prot %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
  )
```

```{r}
sae_prot %>% fit(
  x=xtrain3,
  y=ylabels3,
  epochs = 30,
  batch_size=64,
  validation_split = 0.2
  )
```

```{r}
sae_prot %>%
  evaluate(as.matrix(xtest3), ytestlabels3)
```

Para este modelo, la precisión en la evaluación del conjunto de test es cercana a 0.90.

```{r}
yhat_prot <- predict(sae_prot,as.matrix(xtest3))
```

```{r}
yhatclass_prot<-as.factor(ifelse(yhat_prot<0.5,0,1))
table(yhatclass_prot,  ytestlabels3)
```

Vemos que el porcentaje de error es parecido en ambos casos.

```{r}
confusionMatrix(yhatclass_prot,as.factor(ytestlabels3))
```

Concatenate the 2 models:

```{r}
sae_input_prova1 = layer_input(shape = (ncol(xgene)-1), name = "input_gene_prova")
sae_output_prova1 = sae_input_prova1 %>%
  encoder1() %>%
  encoder2() %>%
  encoder3() %>%
  layer_dense(10,activation = "relu", name='L1_SAE1')%>%
  layer_dense(1,activation = "sigmoid", name='L2_SAE1')
sae_prova1 = keras_model(sae_input_prova1, sae_output_prova1)

sae_input_prova2 = layer_input(shape = 142, name = "input_prot_prova")
sae_output_prova2 = sae_input_prova2 %>%
  encoder1_prot() %>%
  encoder2_prot() %>%
  encoder3_prot() %>%
  layer_dense(10,activation = "relu", name='L1_SAE2')%>%
  layer_dense(1,activation = "sigmoid", name='L2_SAE2')
sae_prova2 = keras_model(sae_input_prova2, sae_output_prova2)

concatenated<-layer_concatenate(list(sae_output_prova1,sae_output_prova2))

model_output_con<-concatenated %>%
  layer_dense(units = 20,"relu") %>%
  layer_dense(units = 1,activation = "sigmoid")

model_final<-keras_model(list(sae_input_prova1, sae_input_prova2), model_output_con)
summary(model_final)
```

En el modelo concatenado tenemos un total de 1004253 parámetros.

```{r}
model_final %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "acc"
)


# training

model_final %>% fit(
  x = list(input_gene_prova = as.matrix(xtrain), input_prot_prova = as.matrix(xtrain3)),
  y = array(ylabels),  epochs = 30, batch_size = 64, validation_split = 0.2
)

```

```{r}
model_final %>%
  evaluate(list(as.matrix(xtest), as.matrix(xtest3)), ylabelstest)
```
En el modelo concatenado, el valor de pérdida es aproximadamente 0.50 y el de accuracy superior a 0.85.

```{r}
yhat_final <- predict(model_final,list(as.matrix(xtest), as.matrix(xtest3)))
```

```{r}
yhatclass_final<-as.factor(ifelse(yhat_final<0.5,0,1))
table(yhatclass_final,  ylabelstest)
```

```{r}
confusionMatrix(yhatclass_final,as.factor(ylabelstest))
```


**9. On the testset, provide the ROC curve and AUC, and compare it with the model found in point 5.**

```{r, results='hide'}
roc_sae_test2 <- roc(response = ylabelstest, predictor =as.vector(yhat_final))
```


```{r}
plot(roc_sae_test2, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

Segun este modelo y el valor obtenido de auc: 0.918, obtenemos que este modelo tiene una precisión alta para nuevos valores.

```{r}
par(mfrow = c(1,2))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
plot(roc_sae_test2, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
par(mfrow = c(1,1))
```

Vemos que en los dos modelos obtenemos valores similares de AUC.

**10. Discuss the results of the analysis.**

Primero de todo tenemos el modelo del apartado 4, donde despues de generar los 3 autoencoders tenemos 995671 parametros. En este modelo, al compilarlo y entrenarlo con 15 "epochs" y un "batch size" de 64, obtenemos que al evaluarlo, el valor de la perdida es de 0.40 y la precision de 0.85. Con esto, podríamos decir que este modelo predice bastante bien.

En el apartado 6 comparamos diferentes capas y vemos como la diferencia entre los modelos es muy baja.

Finalmente, tenemos el modelo combinando el modelo generado con el gene y el modelo generado con el protein, extraido de un ejemplo de clase. En este modelo, la precisión aumenta, aunque la pérdida también lo hace. De todos modos, visto desde un punto de vista estadístico, no vemos una diferencia significativa ya que la precisión que hemos obtenido quedaría dentro del intervalo de confianza de la precisión del primero.

Como reflexión final, hemos visto que los autoencoders nos permiten reducir muchísimo el número de parámetros con los que la red neuronal densa va a trabajar. Al ejecutar el autoencoder, calculamos los pesos y después los congelamos, por lo que a partir de ahí, podemos conseguir trabajar en una dimensionalidad muchísimo más baja y, como hemos visto, obteniendo valores de precisión relativamente altos (cercanos al 90%).